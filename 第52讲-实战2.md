## 剖析搜索引擎背后的数据结构和算法

### 整体介绍

首先搜索引擎需要储存大量的网页作为**资源池**, 那么就需要网络爬虫.一般会选定一些种子网页, 比如一些大的门户网站, 政府网站等

**分析**:爬完之后进行数据清洗, 分词, 根据距离种子网页的距离和关键词出现的次数来定权重(PageRank)

然后将其倒排**索引**(即从原来的网页对应关键词到关键词对应网页)

响应用户**查询**, 对用户关键词进行分词, 然后根据用户的搜索习惯以及已有的倒排索引, 依据权重来返回Page.

#### 搜集

搜索引擎会把整个互联网看成一张有向图, 每个页面都是一个顶点. 如果一个页面包含另一个页面那么就建立一条有向边, 利用图的广度优先搜索来遍历整个互联网中的网页.

主要会涉及一些几个重要的文件:

**links.bin**

广度优先搜索过程中爬虫会不断的解析网页, 也会将网页链接不停推到队列中, 可能会多到内存放不下. 所以我们用一个储存在磁盘中的文件(links.bin)来作为搜索队列, 每次从这个文件里面取页面, 爬去之后再放进队列中(先进先出)

这样有一个好处就是数据持久化, 支持 **断点续爬**, 

**bloom_filter.bin**

布隆过滤器进行判重, 避免页面的重复爬取. 在位图那一讲中已经提及, 假设一亿个网页, 开一亿位二进制, 然后求得多个hash函数的hash值.可以很快速且节省内存的判重, 之所以也放入磁盘中, 还是为了断电之后也能保留这部分内容.可以选择每隔半小时将内存的布隆过滤器放入磁盘中

**doc_raw.bin**

爬取网页后需要将原始网页储存下来(网页快照),以备后面的离线分析, 索引之用.但是海量的数据存储是使用的哪种数据格式呢?如果一个网页对应一个文件, 那么必然会导致生成的文件非常之多, 常用的文件系统不适合储存这么多的文件, 所以可以把多个文件储存在一个文件中, 每个网页直接通过一定标识符进行区分, doc_id是为后续排序用

![](/img/search_engine.jpg)

当然 这样一个文件也不能太大, 文件系统的格式对单个文件的大小也有限制, 可以设置一个值比如1GB, 如果文件已经超过1GB了, 那么就新建一个文件进行添加.

假设硬盘大小位100GB, 一个网页平均大小64KB, 一台机器上可以储存100万到200万左右的网页, 带宽10MB, 下载100GB的网页大约需要10000秒, 也就是爬取100多万个网页只需要几个小时.

**doc_id.bin**
给每个网页分配唯一的ID, 维护一个中心的计数器, 每添加一个网页便将计数器加一, 然后将网页链接跟编号之间的关系储存在doc_id.bin中

**links.bin和bloom_filter.bin是为爬虫自身所用, doc_raw.bin, doc_id是作为搜集成果, 为后面的分析,索引, 查询用**

#### 分析

1. 抽取网页文本信息
- 去掉用户看不见的部分, 比如`<style><script><option>`
- 去掉所有的网页标签

都可以使用ac自动机进行多模式匹配

2. 分词创建临时索引
- 英文网页分析很简单, 空格和标点等作为分隔符
- 中文网页需要一个词库, 包含大量的常用词语, 对词库进行最长匹配, 也就是Trie树结构
- 每个网页会得到一组单词列表, 把单词和列表对应的关系写入一个临时索引(tmp_index.bin)中, 用于构建倒排索引文件.

![](/img/search_engine2.jpg)

在tmp_index.bin中储存的是单词编号, 而不是单词本身, 节省空间. 可以类似网页编号, 给单词维护一个计数器, 每分割出一个新单词时, 取一个编号, 计数器加一. 但是需要用一个散列表来维护编号和单词的关系.在分词的过程中拿分割出来的的单词先在散列表中查找, 找到直接用编号, 没有就去计数器拿号码, 并将号码和单词放入散列表中

处理完所有的网页后将这个编号和单词的对应关系写入磁盘文件:term_id.bin

**分析得到了两个重要文件:tmp_index.bin, term_id.bin**

#### 索引
负责将分析产生的临时索引构建为倒排索引, 记录每个单词包含的网页列表

![](/img/search_engine3.jpg)

这个转换的算法应该如何实现呢?

先对临时索引文件, 按照单词编号大小进行排序, 临时索引较大, 基于内存的排序无法处理, 所以可以使用归并排序, 分割成多个小文件, 先对每个小文件进行排序, 最后合并在一起.实际软件开发中可以利用MapReduce处理.

![](/img/search_engine4.jpg)

除了倒排之外, 还需要一个文件记录每个单词编号在倒排索引的偏移位置, 命名为term_offset.bin, 帮助我们快速定位查找某个单词的编号在倒排索引的位置.

![](/img/search_engine5.jpg)

经过索引得到了两个文件: **index.bin,term_offset.bin**

#### 查询

以上生成的文件都是为了查询所需, 先看看都生成了哪些文件?

- doc_id.bin: 网页和编号对应关系
- term_id.bin: 单词和编号对应关系
- index.bin: 倒排单词对应网页索引
- term_offset.bin: 单词的偏移量

这四个文件除了倒排索引index.bin比较大之外其他的都比较小, 可以加载到内存中, 组织成散列表这种数据结构.

用户在搜索时, 
1. 先进行分词
2. 从term_id查询单词的编号, 
3. 从term_offset查询单词偏移量
4. 从index查询对应网页编号
5. 从doc_id查询具体的网页, 可以统计出现的次数(散列表), 出现次数越多, 说明包含越多的用户查询单词.


#### 总结
整个过程涉及了图, 散列表, Trie树, 布隆过滤器, 单模式字符串匹配, AC自动机, 广度优先遍历, 归并排序等. 之后我会写一个简单的demo.


